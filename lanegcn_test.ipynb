{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "from importlib import import_module\n",
    "from numbers import Number\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler, DataLoader\n",
    "\n",
    "\n",
    "from utils import Logger, load_pretrain\n",
    "\n",
    "from lanegcn import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, Dataset, collate_fn, net, loss, post_process, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(pid):\n",
    "    np_seed = int(pid)\n",
    "    np.random.seed(np_seed)\n",
    "    random_seed = np.random.randint(2 ** 32 - 1)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "dataset = Dataset('./dataset/train_mini/data', config, train=True)\n",
    "train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"workers\"],\n",
    "        shuffle=False,   # True: At each epoch, reorder the data\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn,   # The next 36 were thrown away\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['city', 'orig', 'gt_preds', 'has_preds', 'theta', 'rot', 'feats', 'ctrs', 'graph', 'trajs2', 'traj1'])\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for i, data in enumerate(train_loader):\n",
    "    data = dict(data)\n",
    "    break\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data['city'])\n",
    "# print(len(data['trajs']))\n",
    "# print(len(data['trajs'][0]))\n",
    "# for i in range(32):\n",
    "#     print(dataset[i]['city'])\n",
    "\n",
    "# print(data['trajs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "dict_keys(['idx', 'city', 'feats', 'ctrs', 'orig', 'theta', 'rot', 'gt_preds', 'has_preds', 'graph', 'trajs2', 'traj1'])\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "from argoverse.data_loading.argoverse_forecasting_loader import ArgoverseForecastingLoader\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "\n",
    "# avl = ArgoverseForecastingLoader('./dataset/train_mini/data')\n",
    "# print(avl[0])\n",
    "\n",
    "split = np.load('./dataset/preprocess/train_crs_dist6_angle90.p', allow_pickle=True)\n",
    "\n",
    "print(len(split))\n",
    "# print(split[1])\n",
    "print(split[0].keys())\n",
    "print(split[9]['feats'].size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('./dataset/preprocess/train_crs_dist6_angle90.p', config, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 len =  32\n",
      "i = 1 len =  32\n",
      "i = 2 len =  32\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    data =dict(data) \n",
    "    print('i =', i,'len = ', len(data['city']))\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['city', 'orig', 'gt_preds', 'has_preds', 'theta', 'rot', 'feats', 'ctrs', 'graph', 'trajs2', 'traj1']) 32\n",
      "32 dict_keys(['ctrs', 'num_nodes', 'feats', 'turn', 'control', 'intersect', 'pre', 'suc', 'lane_idcs', 'pre_pairs', 'suc_pairs', 'left_pairs', 'right_pairs', 'left', 'right'])\n",
      "32 torch.Size([12, 20, 3])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys(), len(data['city']))\n",
    "# print(data['gt_preds'])\n",
    "print(len(data['graph']), data['graph'][0].keys())\n",
    "print(len(data['feats']), data['feats'][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MIA', 'MIA', 'PIT', 'PIT', 'MIA']\n"
     ]
    }
   ],
   "source": [
    "print(data['city'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 torch.Size([12, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "print(len(data['gt_preds']), data['gt_preds'][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "dict_keys(['ctrs', 'num_nodes', 'feats', 'turn', 'control', 'intersect', 'pre', 'suc', 'lane_idcs', 'pre_pairs', 'suc_pairs', 'left_pairs', 'right_pairs', 'left', 'right'])\n",
      "torch.Size([1431, 2]) tensor([[ 78.8280, -70.3307],\n",
      "        [ 80.1185, -70.0442],\n",
      "        [ 81.4089, -69.7577],\n",
      "        ...,\n",
      "        [ 95.3565,  71.6838],\n",
      "        [ 95.1766,  72.4025],\n",
      "        [ 94.9967,  73.1211]])\n",
      "1431\n",
      "torch.Size([12, 20, 3])\n",
      "tensor([ 1.0263e+00, -6.4420e-07])\n",
      "tensor([[ 0.0000e+00,  0.0000e+00],\n",
      "        [-1.0247e-01,  5.9214e-01],\n",
      "        [ 7.1112e-01, -4.0717e-01],\n",
      "        [ 4.1534e-01,  4.0864e-02],\n",
      "        [ 4.7127e-01, -2.9227e-01],\n",
      "        [ 1.1025e+00, -3.0220e-01],\n",
      "        [ 4.8492e-01, -1.5746e-01],\n",
      "        [ 6.4992e-01, -1.3299e-01],\n",
      "        [ 6.6781e-01, -1.6725e-01],\n",
      "        [ 4.6423e-01, -8.7786e-02],\n",
      "        [ 3.2890e-01, -8.8570e-02],\n",
      "        [ 1.2687e+00, -1.3822e-01],\n",
      "        [ 5.5424e-01, -5.0619e-02],\n",
      "        [ 5.1367e-01, -5.6269e-02],\n",
      "        [ 1.3857e+00,  1.7025e-02],\n",
      "        [ 4.7146e-01,  7.9946e-02],\n",
      "        [ 7.3161e-01, -3.7486e-02],\n",
      "        [ 7.4345e-01,  1.5936e-01],\n",
      "        [-1.0262e+00, -7.5051e-09],\n",
      "        [ 7.5694e-05, -6.5171e-07]]) torch.Size([20, 2])\n"
     ]
    }
   ],
   "source": [
    "print(len(data['graph']))\n",
    "print(data['graph'][0].keys())\n",
    "print(data['graph'][0]['ctrs'].size(), data['graph'][0]['ctrs'])\n",
    "ctrs0 = data['graph'][0]['ctrs']\n",
    "ctrs1 = data['graph'][1]['ctrs']\n",
    "x0 = ctrs0[:,0]\n",
    "y0 = ctrs0[:,1]\n",
    "x1 = ctrs1[:,0]\n",
    "y1 = ctrs1[:,1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.scatter(x0, y0)\n",
    "# plt.scatter(x1, y1)\n",
    "\n",
    "print(data['graph'][0]['num_nodes'])\n",
    "\n",
    "# print(data['ctrs'][0].size())\n",
    "# plt.scatter(data['ctrs'][0][:,0], data['ctrs'][0][:,1])\n",
    "\n",
    "ctrs = data['ctrs'][0]\n",
    "feat = data['feats'][0]\n",
    "print(data['feats'][0].size())\n",
    "# print(data['feats'][0][0], len(data['feats'][0][0]))\n",
    "\n",
    "print(feat[0][-1, :2])   # ctrs is the last row of feat\n",
    "\n",
    "# print(feat[0][:, :2])\n",
    "# a = feat[0][:,:2].clone()\n",
    "# a[19] = ctrs[0]\n",
    "\n",
    "for j in range(len(ctrs)):\n",
    "    a = feat[j][:,:2].clone()\n",
    "    a[19] = ctrs[j]\n",
    "    for i in range(18):\n",
    "        a[18-i] = a[19-i] - feat[j][19-i,:2]\n",
    "        # plt.scatter(a[:,0], a[:,1])\n",
    "        print(a, a.size())\n",
    "        break\n",
    "    break\n",
    "       \n",
    "# print(a)\n",
    "\n",
    "\n",
    "# traj1 = data['traj1'][0]\n",
    "# x2 = traj1[0][:,0]\n",
    "# y2 = traj1[0][:,1]\n",
    "# print(traj1[0], traj1[0].size())\n",
    "# # plt.scatter(x2,y2)\n",
    "# # print(data['trajs'][0][0])\n",
    "# print(data['gt_preds'][0][0])\n",
    "# plt.scatter(data['trajs'][0][0][:,0],data['trajs'][0][0][:,1])\n",
    "# plt.scatter(data['gt_preds'][0][0][:,0],data['gt_preds'][0][0][:,1])\n",
    "# # print(a - traj1[0][:,:2])\n",
    "# print(traj1[1,:,:2])\n",
    "# # print(type(data['trajs'][0][0]))\n",
    "# traj = torch.cat((data['trajs'][0][0][:,:2].float(),data['gt_preds'][0][0]), dim=0)\n",
    "# print(traj)\n",
    "# plt.scatter(traj[:,0], traj[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actor_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43146/37193168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# trajs = data['trajs'][0][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pre = trajs[18] - orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marctan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m rot = np.asarray([\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre' is not defined"
     ]
    }
   ],
   "source": [
    "orig = data['orig'][0]\n",
    "# trajs = data['trajs'][0][0]\n",
    "# pre = trajs[18] - orig\n",
    "# theta = np.pi - np.arctan2(pre[1], pre[0])\n",
    "\n",
    "rot = np.asarray([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta), np.cos(theta)]], np.float32)\n",
    "\n",
    "trajs = np.matmul(rot, (traj - orig.reshape(-1, 2)).T).T\n",
    "plt.scatter(trajs[:,0], trajs[:,1])\n",
    "plt.scatter(trajs[19,0], trajs[19,1])\n",
    "print(trajs[19,0], trajs[19,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all actoar trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(data['trajs'][0]))\n",
    "print(len(data['gt_preds'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig = data['orig'][0]\n",
    "trajs = data['trajs'][0]\n",
    "pre = trajs[0][18] - orig\n",
    "theta = np.pi - np.arctan2(pre[1], pre[0])\n",
    "\n",
    "rot = np.asarray([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta), np.cos(theta)]], np.float32)\n",
    "\n",
    "trajs = np.matmul(rot, (traj - orig.reshape(-1, 2)).T).T\n",
    "plt.scatter(trajs[:,0], trajs[:,1])\n",
    "plt.scatter(trajs[19,0], trajs[19,1])\n",
    "print(trajs[19,0], trajs[19,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ctrs', 'num_nodes', 'feats', 'turn', 'control', 'intersect', 'pre', 'suc', 'lane_idcs', 'pre_pairs', 'suc_pairs', 'left_pairs', 'right_pairs', 'left', 'right'])\n",
      "torch.Size([1431, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0febb11828>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = data['graph'][0]\n",
    "print(graph.keys())\n",
    "g_ctrs = graph['ctrs']\n",
    "print(g_ctrs.size())\n",
    "\n",
    "plt.scatter(g_ctrs[:,0], g_ctrs[:,1])\n",
    "plt.scatter(trajs[:,0], trajs[:,1])\n",
    "plt.scatter(trajs[19,0], trajs[19,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 695.8059, 3405.3269])\n",
      "tensor([[-1.2956e+01,  1.0185e+00],\n",
      "        [-1.3058e+01,  1.6106e+00],\n",
      "        [-1.2347e+01,  1.2035e+00],\n",
      "        [-1.1932e+01,  1.2443e+00],\n",
      "        [-1.1460e+01,  9.5204e-01],\n",
      "        [-1.0358e+01,  6.4983e-01],\n",
      "        [-9.8731e+00,  4.9239e-01],\n",
      "        [-9.2232e+00,  3.5938e-01],\n",
      "        [-8.5552e+00,  1.9216e-01],\n",
      "        [-8.0910e+00,  1.0441e-01],\n",
      "        [-7.7621e+00,  1.5804e-02],\n",
      "        [-6.4936e+00, -1.2244e-01],\n",
      "        [-5.9393e+00, -1.7308e-01],\n",
      "        [-5.4255e+00, -2.2927e-01],\n",
      "        [-4.0399e+00, -2.1232e-01],\n",
      "        [-3.5682e+00, -1.3231e-01],\n",
      "        [-2.8367e+00, -1.6980e-01],\n",
      "        [-2.0932e+00, -1.0415e-02],\n",
      "        [-1.0261e+00, -1.2989e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 1.8087e+00,  2.7296e-01],\n",
      "        [ 2.6776e+00,  4.0361e-01],\n",
      "        [ 3.7478e+00,  4.7925e-01],\n",
      "        [ 4.7522e+00,  6.5236e-01],\n",
      "        [ 5.6648e+00,  8.3381e-01],\n",
      "        [ 6.6203e+00,  1.0210e+00],\n",
      "        [ 7.7797e+00,  1.2091e+00],\n",
      "        [ 8.7255e+00,  1.3909e+00],\n",
      "        [ 9.8123e+00,  1.6719e+00],\n",
      "        [ 1.0706e+01,  1.8053e+00],\n",
      "        [ 1.1631e+01,  2.0578e+00],\n",
      "        [ 1.2665e+01,  2.2713e+00],\n",
      "        [ 1.3747e+01,  2.4150e+00],\n",
      "        [ 1.4677e+01,  2.6998e+00],\n",
      "        [ 1.6065e+01,  3.0921e+00],\n",
      "        [ 1.6801e+01,  3.1888e+00],\n",
      "        [ 1.7687e+01,  3.4930e+00],\n",
      "        [ 1.9115e+01,  3.8092e+00],\n",
      "        [ 1.9930e+01,  3.9900e+00],\n",
      "        [ 2.0766e+01,  4.1528e+00],\n",
      "        [ 2.1917e+01,  4.4780e+00],\n",
      "        [ 2.3106e+01,  4.7774e+00],\n",
      "        [ 2.3763e+01,  4.8944e+00],\n",
      "        [ 2.4663e+01,  5.1957e+00],\n",
      "        [ 2.5661e+01,  5.3809e+00],\n",
      "        [ 2.6689e+01,  5.7182e+00],\n",
      "        [ 2.7762e+01,  5.9563e+00],\n",
      "        [ 2.8591e+01,  6.0344e+00],\n",
      "        [ 2.9598e+01,  6.3754e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 1.8087e+00,  2.7296e-01],\n",
      "        [ 2.6776e+00,  4.0361e-01],\n",
      "        [ 3.7478e+00,  4.7925e-01],\n",
      "        [ 4.7522e+00,  6.5236e-01],\n",
      "        [ 5.6648e+00,  8.3381e-01],\n",
      "        [ 6.6203e+00,  1.0210e+00],\n",
      "        [ 7.7797e+00,  1.2091e+00],\n",
      "        [ 8.7255e+00,  1.3909e+00],\n",
      "        [ 9.8123e+00,  1.6719e+00],\n",
      "        [ 1.0706e+01,  1.8053e+00],\n",
      "        [ 1.1631e+01,  2.0578e+00],\n",
      "        [ 1.2665e+01,  2.2713e+00],\n",
      "        [ 1.3747e+01,  2.4150e+00],\n",
      "        [ 1.4677e+01,  2.6998e+00],\n",
      "        [ 1.6065e+01,  3.0921e+00],\n",
      "        [ 1.6801e+01,  3.1888e+00],\n",
      "        [ 1.7687e+01,  3.4930e+00],\n",
      "        [ 1.9115e+01,  3.8092e+00],\n",
      "        [ 1.9930e+01,  3.9900e+00],\n",
      "        [ 2.0766e+01,  4.1528e+00],\n",
      "        [ 2.1917e+01,  4.4780e+00],\n",
      "        [ 2.3106e+01,  4.7774e+00],\n",
      "        [ 2.3763e+01,  4.8944e+00],\n",
      "        [ 2.4663e+01,  5.1957e+00],\n",
      "        [ 2.5661e+01,  5.3809e+00],\n",
      "        [ 2.6689e+01,  5.7182e+00],\n",
      "        [ 2.7762e+01,  5.9563e+00],\n",
      "        [ 2.8591e+01,  6.0344e+00],\n",
      "        [ 2.9598e+01,  6.3754e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(orig)\n",
    "print(trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (actor_net): ActorNet(\n",
       "    (groups): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Res1d(\n",
       "          (conv1): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (bn1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (bn2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Res1d(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (bn1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (bn2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Res1d(\n",
       "          (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "          (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (bn1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (bn2): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(32, 64, kernel_size=(1,), stride=(2,), bias=False)\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Res1d(\n",
       "          (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (bn1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (bn2): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Res1d(\n",
       "          (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "          (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (bn1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (bn2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Res1d(\n",
       "          (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (bn1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (bn2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lateral): ModuleList(\n",
       "      (0): Conv1d(\n",
       "        (conv): Conv1d(32, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv1d(\n",
       "        (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv1d(\n",
       "        (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (output): Res1d(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (bn2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (map_net): MapNet(\n",
       "    (input): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (seg): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (fuse): ModuleDict(\n",
       "      (ctr): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (ctr2): ModuleList(\n",
       "        (0): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (left): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (norm): ModuleList(\n",
       "        (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (3): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (pre0): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre1): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre2): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre3): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre4): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre5): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (right): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc0): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc1): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc2): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc3): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc4): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc5): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (a2m): A2M(\n",
       "    (meta): Linear(\n",
       "      (linear): Linear(in_features=132, out_features=128, bias=False)\n",
       "      (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (att): ModuleList(\n",
       "      (0): Att(\n",
       "        (dist): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (ctx): Sequential(\n",
       "          (0): Linear(\n",
       "            (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (agt): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (linear): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Att(\n",
       "        (dist): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (ctx): Sequential(\n",
       "          (0): Linear(\n",
       "            (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (agt): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (linear): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (m2m): M2M(\n",
       "    (fuse): ModuleDict(\n",
       "      (ctr): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (ctr2): ModuleList(\n",
       "        (0): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (left): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (norm): ModuleList(\n",
       "        (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (3): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (pre0): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre1): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre2): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre3): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre4): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (pre5): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (right): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc0): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc1): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc2): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc3): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc4): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (suc5): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (m2a): M2A(\n",
       "    (att): ModuleList(\n",
       "      (0): Att(\n",
       "        (dist): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (ctx): Sequential(\n",
       "          (0): Linear(\n",
       "            (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (agt): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (linear): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Att(\n",
       "        (dist): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (ctx): Sequential(\n",
       "          (0): Linear(\n",
       "            (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (agt): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (linear): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (a2a): A2A(\n",
       "    (att): ModuleList(\n",
       "      (0): Att(\n",
       "        (dist): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (ctx): Sequential(\n",
       "          (0): Linear(\n",
       "            (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (agt): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (linear): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Att(\n",
       "        (dist): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (query): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (ctx): Sequential(\n",
       "          (0): Linear(\n",
       "            (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (agt): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (linear): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pred_net): PredNet(\n",
       "    (pred): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): LinearRes(\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Linear(in_features=128, out_features=60, bias=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LinearRes(\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Linear(in_features=128, out_features=60, bias=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): LinearRes(\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Linear(in_features=128, out_features=60, bias=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): LinearRes(\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Linear(in_features=128, out_features=60, bias=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): LinearRes(\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Linear(in_features=128, out_features=60, bias=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): LinearRes(\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Linear(in_features=128, out_features=60, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (att_dest): AttDest(\n",
       "      (dist): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (agt): Linear(\n",
       "        (linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "        (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (cls): Sequential(\n",
       "      (0): LinearRes(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([552, 3, 20]) 32 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "from lanegcn import actor_gather\n",
    "actors, actor_idcs = actor_gather(data['feats'])\n",
    "print(actors.size(), len(actor_idcs), actor_idcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from fractions import gcd\n",
    "from numbers import Number\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from data import ArgoDataset, collate_fn\n",
    "from utils import gpu, to_long,  Optimizer, StepLR\n",
    "\n",
    "from layers import Conv1d, Res1d, Linear, LinearRes, Null\n",
    "from numpy import float64, ndarray\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lanegcn import ActorNet, MapNet, A2M, M2M, M2A, A2A\n",
    "actor_net = ActorNet(config)\n",
    "map_net = MapNet(config)\n",
    "a2m = A2M(config)\n",
    "m2m = M2M(config)\n",
    "m2a = M2A(config)\n",
    "a2a = A2A(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lanegcn import graph_gather\n",
    "\n",
    "actor_ctrs = data[\"ctrs\"]\n",
    "actors = actor_net(actors)\n",
    "# construct map features\n",
    "graph = graph_gather(to_long(data[\"graph\"]))\n",
    "nodes, node_idcs, node_ctrs = map_net(graph)\n",
    "# actor-map fusion cycle \n",
    "nodes = a2m(nodes, graph, actors, actor_idcs, actor_ctrs)\n",
    "nodes = m2m(nodes, graph)\n",
    "actors = m2a(actors, actor_idcs, actor_ctrs, nodes, node_idcs, node_ctrs)\n",
    "actors = a2a(actors, actor_idcs, actor_ctrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actor_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: 6 torch.Size([552, 60])\n",
      "reg1: torch.Size([552, 6, 60])\n",
      "reg2: torch.Size([552, 6, 30, 2])\n",
      "reg3: torch.Size([552, 6, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "norm = \"GN\"\n",
    "ng = 1\n",
    "n_actor = 128\n",
    "for i in range(6):\n",
    "    pred.append(\n",
    "        nn.Sequential(\n",
    "            LinearRes(n_actor, n_actor, norm=norm, ng=ng),\n",
    "            nn.Linear(n_actor, 2 * config[\"num_preds\"]),\n",
    "            )\n",
    "        )\n",
    "pred = nn.ModuleList(pred)\n",
    "# print(pred)\n",
    "\n",
    "from lanegcn import AttDest\n",
    "att_dest = AttDest(n_actor)\n",
    "cls = nn.Sequential(\n",
    "    LinearRes(n_actor, n_actor, norm=norm, ng=ng),\n",
    "    nn.Linear(n_actor, 1)\n",
    ")\n",
    "\n",
    "preds = []\n",
    "for i in range(len(pred)):\n",
    "    preds.append(pred[i](actors))\n",
    "print('preds:', len(preds), preds[0].size())\n",
    "reg = torch.cat([x.unsqueeze(1) for x in preds], 1)\n",
    "print('reg1:', reg.size())\n",
    "reg = reg.view(reg.size(0), reg.size(1), -1, 2)\n",
    "print('reg2:', reg.size())\n",
    "for i in range(len(actor_idcs)):\n",
    "    idcs = actor_idcs[i]\n",
    "    ctrs = actor_ctrs[i].view(-1, 1, 1, 2)\n",
    "    reg[idcs] = reg[idcs] + ctrs\n",
    "print('reg3:', reg.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([552, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "dest_ctrs = reg[:, :, -1].detach()\n",
    "print(dest_ctrs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = att_dest(actors, torch.cat(actor_ctrs, 0), dest_ctrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1032, -0.3202, -0.1938, -0.1255, -0.3869, -0.1473],\n",
      "        [ 0.1090, -0.3428, -0.2464, -0.1446, -0.4149, -0.0618],\n",
      "        [ 0.1689, -0.1829, -0.0995, -0.0457, -0.3549, -0.0630],\n",
      "        ...,\n",
      "        [ 0.0096, -0.3903, -0.3687, -0.1382, -0.5368, -0.2401],\n",
      "        [-0.0020, -0.3137, -0.2986, -0.1570, -0.4016, -0.2123],\n",
      "        [-0.4301, -0.6936, -0.8220, -0.5573, -0.7819, -0.4986]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cls = cls(feats).view(-1, 6)\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([552, 6])\n",
      "tensor([[ 0.1032, -0.1255, -0.1473, -0.1938, -0.3202, -0.3869],\n",
      "        [ 0.1090, -0.0618, -0.1446, -0.2464, -0.3428, -0.4149],\n",
      "        [ 0.1689, -0.0457, -0.0630, -0.0995, -0.1829, -0.3549],\n",
      "        ...,\n",
      "        [ 0.0096, -0.1382, -0.2401, -0.3687, -0.3903, -0.5368],\n",
      "        [-0.0020, -0.1570, -0.2123, -0.2986, -0.3137, -0.4016],\n",
      "        [-0.4301, -0.4986, -0.5573, -0.6936, -0.7819, -0.8220]],\n",
      "       grad_fn=<SortBackward0>) tensor([[0, 3, 5, 2, 1, 4],\n",
      "        [0, 5, 3, 2, 1, 4],\n",
      "        [0, 3, 5, 2, 1, 4],\n",
      "        ...,\n",
      "        [0, 3, 5, 2, 1, 4],\n",
      "        [0, 3, 5, 2, 1, 4],\n",
      "        [0, 5, 3, 1, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "print(cls.size())\n",
    "cls, sort_idcs = cls.sort(1, descending=True)\n",
    "print(cls, sort_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  2,   2,   2,   2,   2,   2],\n",
      "        ...,\n",
      "        [549, 549, 549, 549, 549, 549],\n",
      "        [550, 550, 550, 550, 550, 550],\n",
      "        [551, 551, 551, 551, 551, 551]])\n",
      "tensor([  0,   0,   0,  ..., 551, 551, 551])\n"
     ]
    }
   ],
   "source": [
    "row_idcs = torch.arange(len(sort_idcs)).long()\n",
    "# print(row_idcs)\n",
    "# print('row_idcs:', row_idcs)\n",
    "# print('row_idcs.view(-1, 1):', row_idcs.view(-1, 1)) \n",
    "print(row_idcs.view(-1, 1).repeat(1, sort_idcs.size(1)))\n",
    "\n",
    "row_idcs = row_idcs.view(-1, 1).repeat(1, sort_idcs.size(1)).view(-1)\n",
    "print(row_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 5,  ..., 1, 4, 2]) 3312\n",
      "2940\n",
      "torch.Size([552, 6, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "sort_idcs = sort_idcs.view(-1)\n",
    "print(sort_idcs, len(sort_idcs))\n",
    "print(490*6)\n",
    "\n",
    "reg = reg[row_idcs, sort_idcs].view(cls.size(0), cls.size(1), -1, 2)\n",
    "print(reg.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(actor_idcs) = 32\n"
     ]
    }
   ],
   "source": [
    "print('len(actor_idcs) =', len(actor_idcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 6, 30, 2])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "out = dict()\n",
    "out['cls'], out['reg'] = [], []\n",
    "for i in range(len(actor_idcs)):\n",
    "    idcs = actor_idcs[i]\n",
    "    ctrs = actor_ctrs[i].view(-1, 1, 1, 2)\n",
    "    out['cls'].append(cls[idcs])\n",
    "    out['reg'].append(reg[idcs])\n",
    "\n",
    "\n",
    "print(out['reg'][0].size())\n",
    "\n",
    "print(len(out['reg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot, orig = data[\"rot\"], data[\"orig\"]\n",
    "        \n",
    "# transform prediction to world coordinates\n",
    "for i in range(len(out[\"reg\"])):\n",
    "    out[\"reg\"][i] = torch.matmul(out[\"reg\"][i], rot[i]) + orig[i].view(1, 1, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PredLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 torch.Size([12, 30, 2])\n",
      "torch.Size([552, 30, 2]) torch.Size([552, 30])\n"
     ]
    }
   ],
   "source": [
    "gt_preds = data['gt_preds']\n",
    "has_preds = data['has_preds']\n",
    "print(len(gt_preds), gt_preds[0].size())\n",
    "\n",
    "reg_loss = nn.SmoothL1Loss(reduction='sum')\n",
    "\n",
    "cls, reg = out['cls'], out['reg']\n",
    "cls = torch.cat([x for x in cls], 0)\n",
    "reg = torch.cat([x for x in reg], 0)\n",
    "gt_preds = torch.cat([x for x in gt_preds], 0)\n",
    "has_preds = torch.cat([x for x in has_preds], 0)\n",
    "print(gt_preds.size(), has_preds.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-670.3817, grad_fn=<SumBackward0>) tensor(57263584., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "1 tensor(0., grad_fn=<CloneBackward0>)\n",
      "2 0\n",
      "3 tensor(0., grad_fn=<CloneBackward0>)\n",
      "4 0\n"
     ]
    }
   ],
   "source": [
    "loss_out = dict()\n",
    "zero = 0.0 * (cls.sum() + reg.sum())\n",
    "print(cls.sum(), reg.sum())\n",
    "print(zero)\n",
    "\n",
    "loss_out['cls_loss'] = zero.clone()\n",
    "loss_out['num_cls'] = 0\n",
    "loss_out['reg_loss'] = zero.clone()\n",
    "loss_out['num_reg'] = 0\n",
    "\n",
    "print('1', loss_out['cls_loss'])\n",
    "print('2', loss_out['num_cls'])\n",
    "print('3', loss_out['reg_loss'])\n",
    "print('4', loss_out['num_reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 30\n",
      "has_preds:\n",
      " tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ..., False, False, False]])\n",
      "last:\n",
      " tensor([[1.0000, 1.0033, 1.0067,  ..., 1.0900, 1.0933, 1.0967],\n",
      "        [1.0000, 1.0033, 1.0067,  ..., 1.0900, 1.0933, 1.0967],\n",
      "        [1.0000, 1.0033, 1.0067,  ..., 1.0900, 1.0933, 1.0967],\n",
      "        ...,\n",
      "        [1.0000, 1.0033, 1.0067,  ..., 0.0900, 0.0933, 0.0967],\n",
      "        [1.0000, 1.0033, 1.0067,  ..., 1.0900, 1.0933, 1.0967],\n",
      "        [1.0000, 1.0033, 1.0067,  ..., 0.0900, 0.0933, 0.0967]])\n",
      "torch.Size([552, 30])\n",
      "last_idcs:\n",
      " tensor([29, 29, 29, 29, 29, 29, 29, 29,  0, 29, 17,  1, 29, 29,  1, 29, 29,  4,\n",
      "        11, 29, 24, 29,  8, 14, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 24,\n",
      "         8, 29, 29, 29, 26, 29, 16, 23, 27, 22, 11, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 17,\n",
      "        29, 29, 29, 29, 29, 29, 29,  0,  9, 29, 17, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29,  0, 22, 19, 29, 29,  0, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29,  1, 29, 29, 29, 20, 29, 26, 25, 28, 29,  0, 29,  9,  5,  6,  5,\n",
      "        29, 29, 29, 29,  3,  1, 29, 10, 29, 29, 18, 29, 22, 29,  5,  1, 29,  5,\n",
      "        29, 20, 29, 29,  0, 29, 29, 29, 29, 16, 29, 29, 16,  0,  5, 11,  4, 29,\n",
      "        10, 29, 29, 29, 29,  9, 29, 10, 29, 29, 29, 29, 29, 29, 26, 29, 29, 29,\n",
      "        29, 29,  9, 18, 29,  0,  9,  2, 29, 29, 29, 14, 13, 14,  1,  1, 15, 29,\n",
      "        12, 15, 12,  0, 29,  2, 29, 29, 29, 29, 29, 29,  8, 23, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29, 21, 29, 29, 29, 29,  1,  2, 29, 29, 29,\n",
      "        29, 15,  2, 29,  2, 29, 10, 29, 10, 29, 29, 29, 29, 29, 29, 29, 29,  1,\n",
      "        29, 29, 29, 25, 29,  0, 29, 13, 29,  1, 10, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 12, 29, 29, 29, 29,  0, 29,  8, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29,  0, 29, 29, 29, 29, 29, 29, 29, 29,  3,\n",
      "         1, 29, 12, 29,  1,  5,  5, 26,  1, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29,  1, 29, 29, 29, 19, 29, 29, 29, 29, 29, 29,  3, 29, 29, 29, 29,\n",
      "         7, 29,  7, 10, 29, 29, 29, 29, 29, 29, 29, 22, 29, 29, 29, 29, 29,  2,\n",
      "        29, 10, 29,  7, 29,  1, 29, 29, 29, 29, 10, 29, 29,  4, 29, 29, 29,  2,\n",
      "         2, 29, 29, 29, 29, 22, 29, 29,  7,  4, 29, 29, 15, 29,  7, 29, 29, 29,\n",
      "        29,  7, 29, 29, 29, 29, 21, 29,  9, 29, 29, 29, 27,  0, 29, 29, 29, 29,\n",
      "        11, 22, 29,  6, 29, 29, 29, 22, 11, 29, 29, 29, 29, 29, 29, 29, 10, 29,\n",
      "        29,  6, 10, 29,  4, 29, 29, 29, 29, 29, 29, 29,  3, 29,  4, 29, 29, 29,\n",
      "        29, 29, 12, 12, 29, 28, 29, 29, 29, 29, 29, 14, 29, 20, 29, 14,  6, 29,\n",
      "        29, 29, 29, 29,  4,  8, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         8, 21, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,  2, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,  5, 29, 29, 11, 29,\n",
      "         9, 29, 29, 29, 29, 29, 10,  1,  0,  2, 29, 29, 29, 29, 29, 26, 29, 29,\n",
      "         6,  0,  2, 29, 29, 29, 29, 29, 23, 13, 29, 12]) 552\n",
      "tensor(29)\n",
      "mask:\n",
      " tensor([ True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True, False, False,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True]) torch.Size([552]) torch.Size([552, 30])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "config[\"pred_size\"] = 30\n",
    "config[\"pred_step\"] = 1\n",
    "config[\"num_preds\"] = config[\"pred_size\"] // config[\"pred_step\"]\n",
    "config[\"num_mods\"] = 6\n",
    "num_mods, num_preds = config['num_mods'], config['num_preds'] \n",
    "print(num_mods, num_preds)\n",
    "\n",
    "last = has_preds.float() + 0.1 * torch.arange(num_preds).float() / float(num_preds)\n",
    "\n",
    "# print('torch.arange:\\n', torch.arange(num_preds))\n",
    "print('has_preds:\\n', has_preds)\n",
    "# print(torch.arange(num_preds).float() / float(num_preds))\n",
    "\n",
    "\n",
    "print('last:\\n', last)\n",
    "print(last.size())\n",
    "\n",
    "max_last, last_idcs = last.max(1)\n",
    "# print('max_last:\\n', max_last, len(max_last))\n",
    "print('last_idcs:\\n', last_idcs, len(last_idcs))\n",
    "\n",
    "print(last_idcs[5])\n",
    "mask = max_last > 1.0\n",
    "\n",
    "print('mask:\\n', mask, mask.size(), last.size())\n",
    "\n",
    "a = 0\n",
    "for i in range(len(mask)):\n",
    "    if mask[i]==False:\n",
    "        a = a + 1\n",
    "        print(has_preds[i])\n",
    "        \n",
    "print(a)    # a = 22\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = cls[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([526, 6])\n"
     ]
    }
   ],
   "source": [
    "print(cls.size())\n",
    "# print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = reg[mask]\n",
    "gt_preds = gt_preds[mask]\n",
    "has_preds = has_preds[mask]\n",
    "last_idcs = last_idcs[mask]\n",
    "\n",
    "row_idcs = torch.arange(len(last_idcs)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "        518, 519, 520, 521, 522, 523, 524, 525]) 526\n"
     ]
    }
   ],
   "source": [
    "print(row_idcs, len(row_idcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg: torch.Size([526, 6, 30, 2])\n",
      "last_idcs: torch.Size([526])\n",
      "gt_preds: torch.Size([526, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "print('reg:', reg.size())\n",
    "print('last_idcs:', last_idcs.size())  # Ture's index\n",
    "print('gt_preds:', gt_preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 torch.Size([526])\n"
     ]
    }
   ],
   "source": [
    "dist = []\n",
    "for j in range(num_mods):\n",
    "    dist.append(\n",
    "        torch.sqrt(\n",
    "            (\n",
    "                (reg[row_idcs, j, last_idcs] - gt_preds[row_idcs, last_idcs])**2\n",
    "            ).sum(1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(len(dist), dist[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([526, 6])\n",
      "torch.Size([526]) torch.Size([526])\n"
     ]
    }
   ],
   "source": [
    "dist = torch.cat([x.unsqueeze(1) for x in dist], 1)\n",
    "print(dist.size())\n",
    "\n",
    "min_dist, min_idcs = dist.min(1)\n",
    "print(min_dist.size(), min_idcs.size())\n",
    "row_idcs = torch.arange(len(min_dist)).long()\n",
    "# print('row_idcs:', row_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_size: torch.Size([526, 6])\n",
      "cls:\n",
      " tensor([[ 0.1032, -0.1255, -0.1473, -0.1938, -0.3202, -0.3869],\n",
      "        [ 0.1090, -0.0618, -0.1446, -0.2464, -0.3428, -0.4149],\n",
      "        [ 0.1689, -0.0457, -0.0630, -0.0995, -0.1829, -0.3549],\n",
      "        ...,\n",
      "        [ 0.0096, -0.1382, -0.2401, -0.3687, -0.3903, -0.5368],\n",
      "        [-0.0020, -0.1570, -0.2123, -0.2986, -0.3137, -0.4016],\n",
      "        [-0.4301, -0.4986, -0.5573, -0.6936, -0.7819, -0.8220]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "cls[row_idcs, min_idcs]:\n",
      " tensor([-3.2015e-01, -3.4281e-01, -1.8294e-01, -1.7384e-02, -2.5417e-01,\n",
      "        -3.0569e-01, -1.8879e-01, -2.5088e-01, -2.3064e-01, -7.2786e-03,\n",
      "        -4.7147e-01,  1.0950e-01,  9.3952e-03,  1.2970e-02, -1.4767e-02,\n",
      "         7.2079e-02, -4.7005e-02, -2.4136e-01, -3.5580e-02, -2.0367e-01,\n",
      "         1.2212e-01,  1.8879e-01, -2.6568e-01,  7.8577e-02,  1.1063e-03,\n",
      "         1.5605e-01, -3.8832e-01, -8.3882e-02,  3.4920e-02,  1.8598e-02,\n",
      "         1.2729e-02,  5.3217e-02,  1.1813e-01, -1.5517e-02, -5.3013e-01,\n",
      "        -3.8612e-01, -9.4932e-02, -1.7738e-01, -1.4071e-01, -3.7386e-01,\n",
      "        -1.7243e-01, -1.7671e-01, -4.0248e-01, -3.0701e-01, -2.1470e-01,\n",
      "        -2.5654e-01,  1.7569e-01,  1.1812e-01,  4.7913e-02,  1.2982e-01,\n",
      "         7.3083e-02,  5.9827e-02,  4.9021e-02,  1.2793e-01,  7.9038e-02,\n",
      "         1.6768e-01,  1.1054e-01,  1.1806e-02,  1.3929e-02, -5.3294e-03,\n",
      "         9.3094e-02,  2.7031e-01,  2.4926e-01,  1.8163e-01,  1.8451e-01,\n",
      "         8.9527e-02,  1.1388e-01,  1.0668e-01,  5.9420e-02,  5.1579e-02,\n",
      "        -3.2474e-01,  6.6889e-02,  8.6761e-02, -4.8194e-02, -4.9240e-01,\n",
      "        -3.4513e-01, -4.8278e-01, -3.5806e-01, -1.7202e-01, -4.7408e-01,\n",
      "        -1.8881e-01, -3.4088e-01, -4.4518e-01, -3.9685e-01, -4.0654e-01,\n",
      "        -3.0522e-01, -6.6668e-01, -4.5571e-01, -4.8441e-01, -4.5050e-01,\n",
      "        -4.3894e-01, -4.4004e-01, -3.8493e-01, -2.3974e-01, -3.9630e-01,\n",
      "        -3.0507e-01, -3.5714e-01, -3.9843e-01, -3.9174e-01, -3.7360e-01,\n",
      "        -4.3816e-01, -3.6495e-01, -4.7049e-01, -5.3602e-01, -3.4603e-01,\n",
      "        -4.8044e-01, -4.0800e-01,  8.1203e-02,  7.3866e-02,  8.8913e-02,\n",
      "        -1.0818e-01,  8.0537e-02, -1.0828e-01, -6.4843e-02, -2.0820e-01,\n",
      "         4.7794e-02,  6.9118e-02, -3.3216e-01, -2.9859e-02, -4.1177e-01,\n",
      "        -2.1852e-01, -1.8144e-01, -4.3471e-02, -2.3625e-01, -2.2382e-01,\n",
      "        -4.1103e-01, -1.1948e-01, -2.4805e-01, -4.9880e-01, -1.9060e-01,\n",
      "        -2.1654e-01, -1.8820e-01, -2.1017e-01, -2.1253e-01, -2.4666e-01,\n",
      "        -1.3112e-01, -9.7289e-02, -1.5182e-01, -1.5618e-01, -2.1596e-01,\n",
      "        -1.3125e-01, -8.9131e-02, -1.4357e-01, -1.4652e-01, -1.3698e-01,\n",
      "        -2.2365e-01,  1.2731e-02, -1.4524e-01, -1.7099e-01,  7.4725e-02,\n",
      "        -3.2248e-01, -4.5589e-01,  8.0708e-03, -2.0404e-01, -4.0601e-01,\n",
      "        -3.4958e-01, -4.4472e-01, -4.7243e-01, -1.9596e-01,  8.7808e-02,\n",
      "         8.8067e-02,  3.5856e-02,  3.3889e-02,  7.0245e-02,  6.8787e-02,\n",
      "        -5.9239e-02,  6.9623e-02,  2.0832e-01,  2.3845e-02,  6.0542e-02,\n",
      "        -2.7801e-01, -3.2268e-01,  1.8597e-01, -4.3686e-01, -4.2914e-01,\n",
      "        -5.7140e-02, -1.2152e-01, -8.9081e-02,  6.5716e-02,  1.2322e-01,\n",
      "         2.3246e-01, -3.2685e-01, -4.2446e-01, -2.2231e-01, -2.8278e-01,\n",
      "        -2.2083e-01, -3.3250e-01,  6.0496e-03, -3.8731e-01, -4.6592e-01,\n",
      "        -1.9405e-01, -1.2390e-01, -1.4167e-01, -1.7286e-01, -2.2027e-01,\n",
      "        -1.8621e-01,  5.6539e-02, -1.1636e-01, -1.9140e-01, -1.8223e-01,\n",
      "        -2.6599e-01, -2.4989e-01, -2.5933e-01, -2.3867e-01, -2.4415e-01,\n",
      "        -2.2632e-01,  3.5244e-02, -2.3927e-01, -3.7736e-01, -3.4378e-01,\n",
      "         5.4033e-02, -2.4680e-01, -2.4607e-01, -3.0871e-01, -4.7857e-01,\n",
      "        -5.6528e-01, -2.4403e-01, -2.9043e-01, -3.0627e-01, -2.6962e-01,\n",
      "         1.3410e-01, -2.1091e-01, -2.0892e-01, -3.9891e-02, -2.4156e-01,\n",
      "        -3.0141e-01, -2.4610e-01, -2.8156e-01, -2.6070e-01, -3.1797e-01,\n",
      "        -3.3109e-01, -3.1169e-01, -3.0299e-01, -2.9215e-01, -3.4276e-01,\n",
      "        -1.8282e-01, -2.2295e-01, -4.6856e-01, -4.5301e-01, -3.5740e-01,\n",
      "        -2.7409e-02, -3.2802e-01, -3.1501e-01, -4.0782e-01, -4.9054e-01,\n",
      "        -4.6683e-01, -2.5298e-01, -1.4555e-01, -5.4746e-02, -7.0551e-02,\n",
      "        -5.9241e-02, -1.6483e-01, -1.0822e-01, -1.0578e-01, -2.2596e-01,\n",
      "        -1.5109e-01, -1.9287e-01, -3.3996e-02, -1.6773e-01, -2.3211e-01,\n",
      "        -1.4461e-01, -1.6402e-01, -1.7426e-01,  6.9624e-02, -1.0344e-01,\n",
      "        -1.6748e-01, -1.4170e-01, -1.9942e-01, -1.3491e-01, -1.5864e-01,\n",
      "        -1.3886e-01, -1.4318e-01, -2.3664e-01, -1.4767e-01, -1.1591e-01,\n",
      "        -2.0627e-01, -1.5744e-01, -1.8224e-01, -2.4103e-01, -7.0060e-01,\n",
      "         5.6540e-02,  4.2804e-02,  1.4574e-02,  1.0529e-02,  2.7821e-01,\n",
      "         1.1155e-02,  1.4201e-02, -4.0637e-01,  4.2042e-02,  4.6280e-02,\n",
      "        -2.3619e-01,  1.8245e-02,  2.4421e-01, -7.1000e-02, -8.2440e-02,\n",
      "        -2.6819e-01, -2.8626e-01,  1.5657e-01,  5.8822e-02,  3.7942e-02,\n",
      "         9.4792e-02,  1.1839e-02,  3.1457e-02,  6.0265e-02,  5.5028e-02,\n",
      "         2.6936e-02,  9.7298e-02,  6.3020e-02,  5.5294e-02,  8.8823e-02,\n",
      "         7.4556e-02,  8.3167e-02, -1.2270e-02,  9.3772e-02,  9.4084e-02,\n",
      "         1.4356e-02,  3.5454e-02,  3.7903e-02, -4.8089e-01, -3.1306e-02,\n",
      "        -1.6562e-01, -7.0602e-02, -5.7082e-02, -1.5131e-01, -6.8762e-02,\n",
      "        -1.5755e-01, -4.7973e-01, -2.5344e-01, -1.6482e-01, -1.1295e-01,\n",
      "        -1.4277e-01, -1.4462e-01, -1.5111e-01, -1.5648e-01,  3.8019e-02,\n",
      "        -1.1015e-01, -2.1332e-01, -1.5285e-01, -2.8595e-01, -9.6038e-02,\n",
      "        -4.6398e-01, -1.9032e-01, -4.6580e-01, -3.9089e-01, -2.7363e-01,\n",
      "        -1.3548e-01, -5.5265e-01,  3.8591e-02, -2.5344e-01, -3.1766e-01,\n",
      "        -4.2898e-01, -3.8579e-01, -4.2264e-01, -4.7203e-01, -3.8674e-01,\n",
      "        -4.0468e-01, -5.2230e-01, -3.0476e-01, -7.7923e-01, -7.0501e-01,\n",
      "        -2.2848e-01, -4.8275e-01, -4.1502e-01, -2.8659e-01, -4.9916e-02,\n",
      "        -4.0327e-01, -3.5662e-01, -1.7784e-01,  4.3934e-02, -3.3757e-01,\n",
      "        -3.9192e-01, -1.7170e-01, -3.6505e-01, -1.7375e-01, -3.7066e-01,\n",
      "        -6.2176e-01, -3.9865e-01, -3.9201e-01, -3.1592e-01, -8.7571e-02,\n",
      "        -1.6941e-01, -1.7197e-01, -1.9047e-01, -3.2039e-01, -3.8663e-02,\n",
      "         6.2995e-02, -1.7675e-01, -2.1835e-01, -1.7715e-01, -1.9231e-01,\n",
      "        -2.0010e-01, -7.2377e-02, -1.3181e-01, -2.3799e-01, -3.0500e-01,\n",
      "        -3.0741e-01, -2.4770e-01, -8.4973e-02, -2.1983e-01, -5.2608e-01,\n",
      "        -2.6828e-01, -4.3077e-01,  5.7256e-02,  7.4491e-02,  3.1387e-02,\n",
      "        -1.3535e-02,  1.1391e-01,  3.3467e-02, -9.4051e-02, -4.4996e-01,\n",
      "         2.7731e-02, -1.2670e-03,  1.4232e-02, -3.6933e-01, -2.3817e-02,\n",
      "        -6.3118e-01,  4.7284e-02,  1.5390e-01,  7.4828e-02,  1.1050e-01,\n",
      "         1.3153e-01, -4.0750e-01, -1.6437e-01, -4.8230e-01, -1.6817e-01,\n",
      "        -1.5591e-01, -1.7043e-01, -2.0738e-01, -1.3751e-01, -1.8518e-02,\n",
      "        -8.0791e-03, -1.3282e-01, -8.5514e-02, -1.4659e-01, -3.5333e-01,\n",
      "        -6.9964e-02, -1.4989e-01, -7.9286e-02, -8.8737e-02, -7.2757e-01,\n",
      "        -2.7177e-01, -5.9367e-01, -3.5563e-01, -3.0086e-01,  9.9459e-02,\n",
      "         5.3550e-02,  2.1686e-02,  2.2692e-03, -4.6463e-04, -1.2406e-01,\n",
      "         4.0762e-02,  7.9203e-02, -5.4769e-02, -1.0668e-01, -1.9416e-01,\n",
      "        -2.4220e-01, -2.5428e-01, -1.8472e-01, -2.6718e-01, -2.0940e-01,\n",
      "        -2.0780e-01, -2.8901e-01, -1.2851e-01, -2.3015e-01,  5.9104e-02,\n",
      "        -2.9518e-01, -1.5119e-01, -2.3105e-01, -2.4766e-01, -3.4443e-01,\n",
      "        -3.1235e-01, -3.1860e-01, -1.1568e-01, -3.3987e-01, -3.1648e-01,\n",
      "        -3.7268e-01, -3.6579e-01, -3.2079e-01, -2.6715e-01, -2.2574e-01,\n",
      "        -4.4715e-01, -2.8510e-01, -1.3963e-01, -1.1104e-01, -1.6265e-01,\n",
      "        -2.6691e-01, -3.1724e-01, -2.1936e-01, -2.9197e-01, -1.9643e-01,\n",
      "        -1.3067e-01, -3.0791e-01, -1.9535e-01, -2.4352e-01, -2.1975e-01,\n",
      "        -3.3805e-01, -4.7872e-01, -2.4959e-01,  1.1341e-01, -7.0802e-02,\n",
      "        -2.1291e-01, -3.8167e-01, -2.0537e-01, -2.6150e-01, -4.4613e-01,\n",
      "        -3.9258e-01, -5.1127e-01, -1.5754e-01, -1.4405e-01, -1.4480e-01,\n",
      "        -1.6940e-01, -1.8314e-01, -3.0149e-01, -2.3332e-01, -2.0195e-01,\n",
      "        -3.0815e-01, -3.1484e-01, -2.1058e-01, -1.1561e-01, -2.4547e-01,\n",
      "        -5.1749e-03, -2.0106e-01, -3.9302e-01,  9.5933e-03, -2.1231e-01,\n",
      "        -5.5728e-01], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('cls_size:', cls.size())\n",
    "print('cls:\\n', cls)   # scores\n",
    "# print('min_idcs:', min_idcs)  # index\n",
    "print('cls[row_idcs, min_idcs]:\\n', cls[row_idcs, min_idcs])  # min_dist scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([526, 1])\n"
     ]
    }
   ],
   "source": [
    "print(cls[row_idcs, min_idcs].unsqueeze(1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mgn: tensor([[-0.4234, -0.1946, -0.1729, -0.1264,  0.0000,  0.0668],\n",
      "        [-0.4518, -0.2810, -0.1982, -0.0964,  0.0000,  0.0721],\n",
      "        [-0.3518, -0.1372, -0.1199, -0.0835,  0.0000,  0.1720],\n",
      "        ...,\n",
      "        [ 0.0000,  0.1478,  0.2497,  0.3782,  0.3999,  0.5464],\n",
      "        [-0.2104, -0.0553,  0.0000,  0.0863,  0.1013,  0.1893],\n",
      "        [-0.1272, -0.0587,  0.0000,  0.1364,  0.2247,  0.2647]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mgn = cls[row_idcs, min_idcs].unsqueeze(1) - cls\n",
    "print('mgn:', mgn)  # min_dist_scores=0, others=min_dist_scores-others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n"
     ]
    }
   ],
   "source": [
    "# config['cls_th']=2\n",
    "mask0 = (min_dist < config['cls_th']).view(-1, 1)  \n",
    "print(mask0)  # if the min_dist < 2, size=[m,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True, False,  True],\n",
      "        [ True,  True,  True,  True, False,  True],\n",
      "        [ True,  True,  True,  True, False,  True],\n",
      "        ...,\n",
      "        [False,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False,  True,  True,  True],\n",
      "        [ True,  True, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "config['cls_ignore'] = 0.2\n",
    "mask1 = dist - min_dist.view(-1, 1) > config['cls_ignore']\n",
    "# K_mods distance > 0.2\n",
    "print(mask1)  # if other_dist-min_dist>0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([526, 6])\n",
      "tensor([[False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False]])\n",
      "torch.Size([526, 6])\n"
     ]
    }
   ],
   "source": [
    "print(mgn.size())\n",
    "print(mask0 * mask1)\n",
    "print((mask0 * mask1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0]) tensor([], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mgn = mgn[mask0 * mask1]   \n",
    "print(mgn.size(), mgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.BoolTensor([[True, False],[False, False]])\n",
    "b = torch.Tensor([[1,2],[3,4]])\n",
    "print(b[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# number of True in [mask0 * mask1] = len(mgn)\n",
    "c = mask0 * mask1\n",
    "d = 0\n",
    "for i in range(c.size(0)):\n",
    "    for j in range(c.size(1)):\n",
    "        if c[i,j] == True:\n",
    "            d = d + 1\n",
    "print(d)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mgn < config['mgn']  # config['mgn']=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], dtype=torch.bool)\n",
      "torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "print(mask)\n",
    "print(mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask.sum= tensor(0)\n",
      "mgn[mask]: tensor([], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('mask.sum=', mask.sum())\n",
    "print('mgn[mask]:', mgn[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config['cls_coef'] = 1.0\n",
      "config['mgn']= 0.2\n",
      "cls_loss= tensor(0., grad_fn=<AddBackward0>)\n",
      "num_cls= 0\n",
      "reg= torch.Size([526, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "coef = config['cls_coef']\n",
    "print(\"config['cls_coef'] =\", coef)\n",
    "print(\"config['mgn']=\", config['mgn'])\n",
    "\n",
    "loss_out['cls_loss'] += coef * (config['mgn'] * mask.sum() - mgn[mask].sum())\n",
    "print('cls_loss=', loss_out['cls_loss'])\n",
    "\n",
    "loss_out['num_cls'] += mask.sum().item()\n",
    "print('num_cls=', loss_out['num_cls'])   # number of min_dis - other socres < 0.2\n",
    "\n",
    "reg = reg[row_idcs, min_idcs]   # only min_dis\n",
    "print('reg=', reg.size())\n",
    "# loss_out['reg_loss'] += coef * reg_loss(reg[has_preds], gt_preds[has_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config[reg_coef]= 1.0\n",
      "reg_loss= tensor(93840088., grad_fn=<AddBackward0>)\n",
      "num_reg= 26226\n"
     ]
    }
   ],
   "source": [
    "coef = config['reg_coef']\n",
    "print('config[reg_coef]=', coef)\n",
    "loss_out['reg_loss'] += coef * reg_loss(reg[has_preds], gt_preds[has_preds])\n",
    "print('reg_loss=', loss_out['reg_loss'])\n",
    "loss_out['num_reg'] += has_preds.sum().item()\n",
    "print('num_reg=', loss_out['num_reg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43146/94512477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "print(output.keys())\n",
    "print(len(output['cls']), output['cls'][0].size())\n",
    "print(len(output['reg']), output['reg'][0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SocialGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp(dim_list, activation='relu', batch_norm=True, dropout=0):\n",
    "    layers = []\n",
    "    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\n",
    "        layers.append(nn.Linear(dim_in, dim_out))\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(dim_out))\n",
    "        if activation == 'relu':\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9793/2702812952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m encoder = Encoder(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mh_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mmlp_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            h_dim=h_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "real_classifier_dims = [h_dim, mlp_dim, 1]\n",
    "\n",
    "real_classifier = make_mlp(\n",
    "            real_classifier_dims,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout)\n",
    "            \n",
    "classifier_input = final_h.squeeze()\n",
    "scores = self.real_classifier(classifier_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_len, pred_len, embedding_dim=64, h_dim=64, mlp_dim=1024,\n",
    "        num_layers=1, activation='relu', batch_norm=True, dropout=0.0,\n",
    "        d_type='local'\n",
    "    ):\n",
    "        super(TrajectoryDiscriminator, self).__init__()\n",
    "\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = obs_len + pred_len\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.d_type = d_type\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            h_dim=h_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        real_classifier_dims = [h_dim, mlp_dim, 1]\n",
    "        self.real_classifier = make_mlp(\n",
    "            real_classifier_dims,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        if d_type == 'global':\n",
    "            mlp_pool_dims = [h_dim + embedding_dim, mlp_dim, h_dim]\n",
    "            self.pool_net = PoolHiddenNet(\n",
    "                embedding_dim=embedding_dim,\n",
    "                h_dim=h_dim,\n",
    "                mlp_dim=mlp_pool_dims,\n",
    "                bottleneck_dim=h_dim,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm\n",
    "            )\n",
    "\n",
    "    def forward(self, traj, traj_rel, seq_start_end=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - traj: Tensor of shape (obs_len + pred_len, batch, 2)\n",
    "        - traj_rel: Tensor of shape (obs_len + pred_len, batch, 2)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch\n",
    "        Output:\n",
    "        - scores: Tensor of shape (batch,) with real/fake scores\n",
    "        \"\"\"\n",
    "        final_h = self.encoder(traj_rel)\n",
    "        # Note: In case of 'global' option we are using start_pos as opposed to\n",
    "        # end_pos. The intution being that hidden state has the whole\n",
    "        # trajectory and relative postion at the start when combined with\n",
    "        # trajectory information should help in discriminative behavior.\n",
    "        if self.d_type == 'local':\n",
    "            classifier_input = final_h.squeeze()\n",
    "        else:\n",
    "            classifier_input = self.pool_net(\n",
    "                final_h.squeeze(), seq_start_end, traj[0]\n",
    "            )\n",
    "        scores = self.real_classifier(classifier_input)\n",
    "        return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('lanegcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ff309fca0a5e8ad5576d7cf0da1773e4040ba62bb168e6dfb3b17a889a1e8a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
